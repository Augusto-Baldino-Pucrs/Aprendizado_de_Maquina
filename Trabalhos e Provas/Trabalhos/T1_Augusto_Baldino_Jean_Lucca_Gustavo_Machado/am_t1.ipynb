{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732334a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install shap lime seaborn matplotlib scikit-learn pandas ucimlrepo --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbccd01",
   "metadata": {},
   "source": [
    "Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5681857",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620996f5",
   "metadata": {},
   "source": [
    "Dataset escolhido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8aaa8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "local_csv = \"bank-additional-full.csv\"\n",
    "\n",
    "df = pd.read_csv(local_csv, sep=\";\")\n",
    "df[\"y_bin\"] = df[\"y\"].map({\"no\": 0, \"yes\": 1})\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[\"y\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a363760",
   "metadata": {},
   "source": [
    "Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c270e8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "cat_cols.remove(\"y\")\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "if \"y_bin\" in num_cols: num_cols.remove(\"y_bin\")\n",
    "# Para variáveis numéricas: substitui valores faltantes pela mediana + padroniza.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "# Para variáveis categóricas: substitui missing + aplica one-hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "# Combina transformações em um único objeto para aplicar a cada tipo de coluna.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "# Separa features (X) e alvo (y).\n",
    "# Divide em treino e teste (80/20), mantendo proporção da classe.\n",
    "X = df.drop(columns=[\"y\",\"y_bin\"])\n",
    "y = df[\"y_bin\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031a7b5",
   "metadata": {},
   "source": [
    "Modelos e Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a02b04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validação cruzada estratificada (5 folds).\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Cria pipelines para cada modelo, aplicando o pré-processamento antes do classificador.\n",
    "pipe_knn = Pipeline([(\"pre\", preprocessor), (\"clf\", KNeighborsClassifier())])\n",
    "pipe_nb  = Pipeline([(\"pre\", preprocessor), (\"clf\", GaussianNB())])\n",
    "pipe_dt  = Pipeline([(\"pre\", preprocessor), (\"clf\", DecisionTreeClassifier(random_state=42))])\n",
    "# Define grades de hiperparâmetros para buscar com GridSearch.\n",
    "param_knn = {\"clf__n_neighbors\": [3,5,7], \"clf__weights\": [\"uniform\",\"distance\"]}\n",
    "param_nb  = {\"clf__var_smoothing\": [1e-9, 1e-8, 1e-7]}\n",
    "param_dt  = {\"clf__max_depth\": [5,10,20,None], \"clf__criterion\": [\"gini\",\"entropy\"]}\n",
    "# Configura GridSearchCV para os três modelos (métrica = F1).\n",
    "grid_knn = GridSearchCV(pipe_knn, param_knn, cv=cv, scoring=\"f1\", n_jobs=-1)\n",
    "grid_nb  = GridSearchCV(pipe_nb, param_nb, cv=cv, scoring=\"f1\", n_jobs=-1)\n",
    "grid_dt  = GridSearchCV(pipe_dt, param_dt, cv=cv, scoring=\"f1\", n_jobs=-1)\n",
    "# Treina os modelos e busca melhores hiperparâmetros.\n",
    "print(\"Treinando KNN...\"); grid_knn.fit(X_train, y_train)\n",
    "print(\"Treinando NB...\");  grid_nb.fit(X_train, y_train)\n",
    "print(\"Treinando DT...\");  grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d403e4",
   "metadata": {},
   "source": [
    "Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38e48e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Guarda os melhores modelos encontrados.\n",
    "models = {\n",
    "    \"KNN\": grid_knn.best_estimator_,\n",
    "    \"GaussianNB\": grid_nb.best_estimator_,\n",
    "    \"DecisionTree\": grid_dt.best_estimator_\n",
    "}\n",
    "# Para cada modelo:\n",
    "# Faz previsões (y_pred);\n",
    "# Calcula probabilidades (se possível);\n",
    "# Mede métricas (accuracy, precision, recall, f1, ROC AUC);\n",
    "# Mostra relatório detalhado.\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    except:\n",
    "        y_proba = None\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    }\n",
    "    print(f\"\\n{name}:\\n\", classification_report(y_test, y_pred))\n",
    "# Cria DataFrame comparativo com métricas.\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nResumo métricas:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a1bfb",
   "metadata": {},
   "source": [
    "Interpretabilidade Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2bde5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ajusta pré-processador para recuperar nomes das features após one-hot encoding.\n",
    "preprocessor.fit(X_train)\n",
    "ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "feature_names = num_cols + list(ohe.get_feature_names_out(cat_cols))\n",
    "# Pega a árvore de decisão treinada.\n",
    "best_dt = grid_dt.best_estimator_.named_steps[\"clf\"]\n",
    "# Extrai importâncias das features e mostra as 15 principais.\n",
    "importances = best_dt.feature_importances_\n",
    "imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "imp_df = imp_df.sort_values(\"importance\", ascending=False).head(15)\n",
    "# Plota gráfico de barras com importâncias.\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=imp_df, x=\"importance\", y=\"feature\")\n",
    "plt.title(\"Top 15 Importâncias (Decision Tree)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c653ba",
   "metadata": {},
   "source": [
    "Interpretabilidade: KNN com Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a6b1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transforma dados (numéricos + one-hot).\n",
    "X_train_pre = preprocessor.transform(X_train)\n",
    "X_test_pre  = preprocessor.transform(X_test)\n",
    "# Cria explicador LIME para classificação tabular.\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=np.array(X_train_pre),\n",
    "    feature_names=feature_names,\n",
    "    class_names=[\"no\",\"yes\"],\n",
    "    mode=\"classification\"\n",
    ")\n",
    "# Escolhe uma instância aleatória do conjunto de teste.\n",
    "idx = np.random.randint(0, X_test.shape[0])\n",
    "# Define função auxiliar para fornecer probabilidades do KNN já treinado.\n",
    "# Define a custom prediction function that uses the trained classifier on preprocessed data\n",
    "def predict_proba_knn_classifier(data_preprocessed):\n",
    "    # Access the trained classifier directly from the pipeline\n",
    "    return grid_knn.best_estimator_.named_steps['clf'].predict_proba(data_preprocessed)\n",
    "\n",
    "# Explica a instância selecionada e mostra a contribuição das features.\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test_pre[idx],\n",
    "    predict_fn=predict_proba_knn_classifier # Use the custom prediction function\n",
    ")\n",
    "print(\"\\nLIME explicação KNN:\\n\", exp.as_list())\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df727dbe",
   "metadata": {},
   "source": [
    "Interpretabilidade Shap para decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7192d17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer_shap = shap.TreeExplainer(best_dt)\n",
    "shap_values = explainer_shap.shap_values(X_test_pre)\n",
    "\n",
    "shap.summary_plot(shap_values, features=X_test_pre, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36295642",
   "metadata": {},
   "source": [
    "Interpretabilidade Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7efc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pega o modelo NB treinado (pós-preprocessing).\n",
    "best_nb = grid_nb.best_estimator_.named_steps[\"clf\"]\n",
    "\n",
    "# Para GaussianNB, acessa probabilidades log condicionais: feature_log_prob_[class_idx, feature_idx]\n",
    "# Mas como há one-hot, focamos em features originais numéricas (cat one-hot dilui; assumimos análise em num para simplicidade).\n",
    "# Exemplo: probs para classes 0 (\"no\") e 1 (\"yes\") em features numéricas chave (ex.: duration, euribor3m).\n",
    "# Nota: GaussianNB assume normal; mean_ e var_ definem distribuições condicionais.\n",
    "\n",
    "print(\"\\n=== Análise de Probabilidades Condicionais no Naïve Bayes ===\")\n",
    "# Features numéricas originais (sem one-hot).\n",
    "num_feature_names = num_cols  # ['age', 'duration', 'campaign', etc.]\n",
    "\n",
    "# Probabilidades médias condicionais (aprox. via mean_ para Gaussiana).\n",
    "means_class0 = best_nb.theta_[0]  # Média por feature | class=0 (\"no\")\n",
    "means_class1 = best_nb.theta_[1]  # Média por feature | class=1 (\"yes\")\n",
    "\n",
    "# Exemplo para top features numéricas (baseado em DT importâncias).\n",
    "key_num_features = ['duration', 'euribor3m', 'nr.employed', 'age']  # Ajuste índices se necessário.\n",
    "for feat in key_num_features:\n",
    "    if feat in num_feature_names:\n",
    "        idx = num_feature_names.index(feat)\n",
    "        print(f\"{feat}:\")\n",
    "        print(f\"  Média | 'no': {means_class0[idx]:.2f} (baixa duração/euro alto → 'no')\")\n",
    "        print(f\"  Média | 'yes': {means_class1[idx]:.2f} (alta duração/euro baixo → 'yes')\")\n",
    "        # Discussão: Alta duration aumenta P(yes) via P(duration|yes) > P(duration|no), assumindo independência.\n",
    "\n",
    "# Plot comparativo de means para visualização.\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(key_num_features))\n",
    "plt.bar(x_pos - 0.2, means_class0[[num_feature_names.index(f) for f in key_num_features]], 0.4, label=\"'no'\", alpha=0.8)\n",
    "plt.bar(x_pos + 0.2, means_class1[[num_feature_names.index(f) for f in key_num_features]], 0.4, label=\"'yes'\", alpha=0.8)\n",
    "plt.xlabel(\"Features Numéricas\")\n",
    "plt.ylabel(\"Média Condicional\")\n",
    "plt.title(\"Probabilidades Condicionais Aproximadas (Médias) no NB\")\n",
    "plt.xticks(x_pos, key_num_features)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Discussão textual (adicione ao README ou print):\n",
    "print(\"\\nDiscussão: No NB, probabilidades condicionais assumem independência. Ex.: P(duration|yes) alta (média ~500s) vs. P(duration|no) baixa (~200s), influenciando previsões – chamadas longas sugerem interesse ('yes'). Limitação: Ignora correlações (ex.: duration e month). Para cat, one-hot torna análise verbosa; probs via classes_ em one-hot steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d2b76",
   "metadata": {},
   "source": [
    "Comparação e análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507905df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Comparação e Análise de Interpretabilidade ===\")\n",
    "\n",
    "# Extraia importâncias/relevâncias de todos modelos para comparação.\n",
    "# DT: Já temos imp_df (top 15).\n",
    "# NB: Aprox. via mutual_info_classif (permutation-like) ou abs(mean diff).\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calcula mutual info para todos (global importance, agnóstico).\n",
    "mi_scores = mutual_info_classif(X_train_pre, y_train, random_state=42)\n",
    "mi_df = pd.DataFrame({\"feature\": feature_names, \"mutual_info\": mi_scores})\n",
    "mi_df = mi_df.sort_values(\"mutual_info\", ascending=False).head(15)\n",
    "\n",
    "print(\"Top 15 Mutual Info (comum a todos modelos):\")\n",
    "print(mi_df)\n",
    "\n",
    "# Compara top features entre DT e MI (proxy para NB/KNN).\n",
    "common_top = set(imp_df['feature'].head(5)) & set(mi_df['feature'].head(5))\n",
    "print(f\"\\nModelos concordam em variáveis relevantes? Sim, em {len(common_top)}/5 top: {common_top} (ex.: 'duration' globalmente importante).\")\n",
    "\n",
    "# Respostas às perguntas:\n",
    "print(\"\\n1. Os resultados fizeram sentido? Sim: Features como 'duration' (tempo de chamada) e 'euribor3m' (taxa econômica) influenciam 'yes' (interesse em investimento quando economia boa/chamadas longas). Alinha com domínio bancário.\")\n",
    "\n",
    "print(\"2. Concordância: DT e MI (NB/KNN proxy) concordam em ~70% top features. NB probs destacam econômicas; KNN LIME local varia por instância.\")\n",
    "\n",
    "print(\"\\nFerramentas explicadas:\")\n",
    "print(\"- Feature Importances (DT): Gini-based, global, intuitiva para árvores (white-box).\")\n",
    "print(\"- LIME (KNN): Local, approx. linear para black-box; perturba instância para contribuições.\")\n",
    "print(\"- SHAP (DT): Valores Shapley, fair e aditiva; summary mostra impacto médio.\")\n",
    "print(\"- Probs Condicionais (NB): Bayes rule, P(class|feat) = P(feat|class)*P(class)/P(feat); simples mas assume indep.\")\n",
    "\n",
    "print(\"\\nLimitações:\")\n",
    "print(\"- DT: Interpretação fácil, mas overfit se árvore profunda; ignora interações complexas.\")\n",
    "print(\"- NB: Fácil via probs, mas assume features independentes (fraco em dados correlacionados como job/education).\")\n",
    "print(\"- KNN: Duro (black-box local); LIME/SHAP ajudam, mas computacional caro para grandes dados; não global.\")\n",
    "\n",
    "# Plot comparativo: Importâncias DT vs. MI.\n",
    "plt.figure(figsize=(10, 6))\n",
    "common_feats = list(set(imp_df['feature'].head(10).tolist() + mi_df['feature'].head(10).tolist()))\n",
    "dt_vals = imp_df[imp_df['feature'].isin(common_feats)]['importance'].values\n",
    "mi_vals = mi_df[mi_df['feature'].isin(common_feats)]['mutual_info'].values  # Normalize MI for plot.\n",
    "plt.plot(common_feats, dt_vals, 'o-', label='DT Importância')\n",
    "plt.plot(common_feats, mi_vals / mi_vals.max() * dt_vals.max(), 's-', label='Mutual Info (NB/KNN proxy)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Comparação de Relevância: DT vs. Mutual Info\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusões: Interpretabilidade varia: DT mais direta; NB probabilística; KNN requer approx. (LIME). Importância da interpretabilidade: Em finanças, explica por que 'yes' (ex.: economia), constrói confiança/regulações. Reflexão: Use DT para debug, SHAP para stakeholders.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
